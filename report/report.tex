%% vi: set tabstop=2, set textwidth=80

\documentclass[11pt]{article}

\usepackage{homework}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage[english]{babel}

% The report should be a description of your work during the lab sessions,
% focussing on the mean shift tracker. You should describe what you did, what you
% noticed and what you could have done different or could have improved. As you
% implemented the tracker you noticed different behaviour if you changed parts of
% the tracker, e.g. a different colour space or the number of bins in the
% histogram. Hopefully these insights have improved your tracker. This is why it
% is essential that you test the tracker also on a video of a domain other than
% soccer (or any other sport on a green field). This other domain will show how
% your implementation depends on the soccer domain. Observe how you can improve
% your design and them describe how you implemented this change or, for lack of
% time, describe how you would change your design. 

\title{Intelligent Multimedia Systems \\ Mean Shift Tracker and Color Models}

\author{F. Huizinga [0418862], B. Stoeller [0426857] \\
      \{folkerthuizinga,bramstoeller\}@gmail.com}

\date{December 27, 2009}

\begin{document}
\maketitle

\begin{abstract}
This report describes the implementation and results of a mean shift tracker
using the Epanechnikov kernel and various color space models. Furthermore, we
perform various analyses on the tracker and color models with the use of two
videos within different domains.
\end{abstract}


\section{Introduction} \label{sec:intro}
An object tracker consists of two major components, the \emph{Object Model} and
the \emph{Tracking Algorithm}. The object model is represented using a
histogram in some color space. The tracking algorithm is the Mean Shift
Algorithm~\cite{kernel-basedobject, real-timetracking}. This report compares
five different color spaces to represent the Object Model and their performance
is measured using two videos in different domains (sport and nature). The
report is organized as follows:  Section \ref{sec:implementation} describes the
implementation of the algorithm. The experiments and results are shown in
Section \ref{sec:results}. Section \ref{sec:conclusion} presents our
conclusions, and finally possible improvements are shown in Section
\ref{sec:future}.


\section{Color Spaces} \label{sec:color}
In this paper we compare the use of several color spaces. For the conveinience
of the reader we will give a short outline of the color spaces we used. All of
them are described in much more detail in \cite{Gevers}.

\subsection{\textit{RGB} - Red Green Blue}
The $R$, $G$ and $B$  color features correspond to the primary colors red,
green and blue as they are observed by humans and most digital cameras. The
$RGB$ color space is dependent on the intensity, color and direction of the
illumination. It is therefore sensitive to shadows and global illumination
changes.

\subsection{\textit{rgb} - Normalized Red Green Blue}
Again the $r$, $g$ and $b$  color features correspond to the primary colors.
However in the $rgb$ space they define the ratio or $R$, $G$ and $B$ rather than
the absolute intensity of $R$, $G$ and $B$. $r$, $g$ and $b$ are calculated by
dividing all three channels $R$, $G$ and $B$ by their total sum (See equation
\ref{eq:r} - \ref{eq:b}). A rather problematic side effect of this dividion is
that normalized $rgb$ colors become unstable and thereby meaningless when the
intensity is small. Because $r$, $g$ and $b$ are normalized, by definition they
sum up to one. This means we are allowed to neglect one of the channels (e.g.
the blue channel) because it's information is hidden in the other two channels.
This has notable computational benefits. Because it changes the order of the
color space from $n^3$ to $n^2$.

\begin{equation}
  r = \frac{R}{R+G+B}
  \label{eq:r}
\end{equation}
\begin{equation}
  g = \frac{G}{R+G+B}
  \label{eq:g}
\end{equation}
\begin{equation}
  b = \frac{B}{R+G+B}
  \label{eq:b}
\end{equation}

\subsection{\textit{HSV} - Hue Saturation Value}

\section{Implementation} \label{sec:implementation}
For our implementation we used the Matlab programming language. We describe the
implementation of the tracking algorithm and the object model separately.

\subsection{Object Model} \label{sec:model}
The object model is represented as a 2D or 3D histogram on the following color
spaces: $rgb$, $xyz$, $RGB$, $HSV$ and $XYZ$. We decided to use a fixed number
of total bins (i.e. the number of bins is not affected by the number of
dimensions).  More formally, if $N$ is the total number of bins, then the number
of bins $b$ for each dimension $i$ is $b_i = N^{1/d}$, where $d$ is the total
number of dimensions. 

\subsection{Tracking Algorithm} \label{sec:mst}
Algorithm \ref{alg:mst} shows our toplevel implementation of the mean shift
tracking algorithm, for the complete details we refer the reader to the
documented sourcecode\footnote{The sourcecode can be found at
http://github.com/Error323/ims}.

\begin{figure}
\centering
\subfigure[The sports domain, a soccer match.]{
\includegraphics[height=5.0cm]{img/soccer_orange_scene}
\label{fig:1a}
}
\subfigure[The nature domain, a hunting cheetah.]{
\includegraphics[height=5.0cm]{img/earth_cheetah_scene}
\label{fig:1b}
}
\caption{\ref{fig:1a} shows the sport domain where we track an orange soccer
player on the left. \ref{fig:1b} shows a scene from the movie Earth, where we
track the head of a cheetah chasing its prey.}
\label{fig:videos}
\end{figure}

\begin{algorithm}
	\caption{MeanShiftTracker($V$, $n$)}
	\begin{algorithmic}[1]
	\REQUIRE The video $V$ with $n$ frames
	\STATE $f_1 \leftarrow V[1]$ \COMMENT{Set the current frame}
	\STATE $\mathbf{y_0} \leftarrow \mathcal{S}(f_1)$ \COMMENT{Select target, store location}
	\STATE $\mathbf{q} \leftarrow \mathcal{M}(f_1, \mathbf{y_0})$ \COMMENT{Create the target model}
	\FOR{$i = 1$ to $n$}
		\STATE $f_i \leftarrow V[i]$
		\STATE $\mathbf{p_0} \leftarrow \mathcal{M}(f_i, \mathbf{y_0})$
		\STATE $bc_0 \leftarrow \mathcal{B}(\mathbf{p_0}, \mathbf{q})$ \COMMENT{Compute Bhattacharyya coefficient}
		\WHILE{true}
			\STATE $\mathbf{W} \leftarrow \mathcal{W}(f_i, \mathbf{y_0}, \mathbf{q}, \mathbf{p_0})$ \COMMENT{Compute the weights matrix}
			\STATE $\mathbf{y_1} \leftarrow \mathbf{y_0} + \mathcal{L}(\mathbf{W})$ \COMMENT{Compute the mean shift}
			\STATE $\mathbf{p_1} \leftarrow \mathcal{M}(f_i, \mathbf{y_1})$
			\STATE $bc_1 \leftarrow \mathcal{B}(\mathbf{p_1}, \mathbf{q})$
			\WHILE{$bc_1 < bc_0$} 
				\STATE $\mathbf{y_1} \leftarrow \frac{1}{2} \dot (\mathbf{y_0} + \mathbf{y_1})$ \COMMENT{Interpolate $\mathbf{y_1}$ if we shifted too far}
				\STATE $\mathbf{p_1} \leftarrow \mathcal{M}(f_i, \mathbf{y_1})$
				\STATE $bc_1 \leftarrow \mathcal{B}(\mathbf{p_1}, \mathbf{q})$
			\ENDWHILE
			\STATE $\mathbf{p_0} \leftarrow \mathbf{p_1}$
			\STATE $\mathbf{y_0} \leftarrow \mathbf{y_1}$
			\STATE $bc_0 \leftarrow bc_1$
			\IF{$\| \mathbf{y_1} - \mathbf{y_0} \| < \epsilon$}
				\STATE break \COMMENT{Break if locations are nearly equal}
			\ENDIF
		\ENDWHILE
	\ENDFOR
	\medskip
	\end{algorithmic}
\label{alg:mst}
\end{algorithm}

\section{Experiments and Results} \label{sec:results}
For our experiments we applied the tracker to videos within two different
domains, nature and sports. Figure \ref{fig:videos} shows the tracker in action
on both domains. In both videos a distinctive object was tracked using the various color models
and for each color model various histogram sizes, see Table \ref{tbl:bins}.

\begin{table}[h!]
\centering
\begin{tabular}{c|c|c}
$N$   & $N^{1/2}$ & $N^{1/3}$\\\hline\hline
$2^6$ & $2^3$     & $2^2$\\\hline
$3^6$ & $3^3$     & $3^2$\\\hline
$4^6$ & $4^3$     & $4^2$\\\hline
$5^6$ & $5^3$     & $5^2$\\
\end{tabular}
=
\begin{tabular}{r|r|r}
$N$     & $N^{1/2}$ & $N^{1/3}$\\\hline\hline
$64$    & $8$       & $4$\\\hline
$729$   & $27$      & $9$\\\hline
$4096$  & $64$      & $16$\\\hline
$15625$ & $125$     & $25$\\
\end{tabular}
\caption{Histogram sizes (i.e. number of bins).}
\label{tbl:bins}
\end{table}

For a (somewhat) objective analysis of the various color models and various
histogram sizes, we chose to manually annotate the position of the target
objects within the frame sequences of each video. In figures \ref{fig:soccer}
and \ref{fig:cheetah} we compared the performance of
the five different color models for four different numbers of bins on the soccer
movie when tracking the orange player and cheetah movie when tracking the
cheetah's head respectively.

Both figures show our final results displayed in
four graphs with frame numbers on the $x$-axis and the Euclidean distance,
between the tracker's output and our labelled data, on the $y$-axis. All graphs
show a very distinctive pattern on the color models that lost track. In the
sports domain (figure \ref{fig:soccer}) this is the result of occlusion caused by
a white player. The white player walks in front of the orange player (the
target object) and pushes the model away at which point it finds a new orange
player and starts tracking him. In the nature domain (figure \ref{fig:cheetah})
if the tracker gets lost this means that some portion of the background is
classified as the target object and the tracker starts tracking that region
instead.\\

The mean Euclidean error between the tracked target object and the ground truth
we annotated by hand is shown in table \ref{tbl:error}. Using the normalized
$rgb$ color space results in the highest accuracy for all our test cases.
This indicates that the $rgb$ color model is a robust one and would indeed be
our choice for object tracking. However we should note that running tests in two
domains might not be sufficient to determine the significance of these results.

\begin{table}[h!]
\centering
\begin{tabular}{r||r|r|r|r|r}
	      & $RGB$ & $rgb$ & $HSV$ & $XYZ$ & $xyz$ \\ \hline \hline
	   64 &  51.8 &   6.8 &   8.1 & 114.2 &  81.3 \\ \hline
	  729 &  57.9 &   5.7 &   5.7 &  45.8 &  66.5 \\ \hline
	 4096 &  31.5 &   4.8 &  56.8 &  58.4 &   5.5 \\ \hline
	15625 &  44.8 &   4.4 &   4.5 &  57.9 &   4.1 \\ \hline
\end{tabular}
\caption{Mean Euclidean error derived from 201 frames from the sports domain and
300 frames from the nature domain.}
\label{tbl:error}
\end{table}

\begin{figure}[h!]
\centering
\subfigure[$64$ bins]{
\includegraphics[height=6.0cm]{img/soccer_orange_64}
\label{fig:3a}
}
\subfigure[$729$ bins]{
\includegraphics[height=6.0cm]{img/soccer_orange_729}
\label{fig:3b}
}
\\
\subfigure[$4096$ bins]{
\includegraphics[height=6.0cm]{img/soccer_orange_4096}
\label{fig:3c}
}
\subfigure[$15625$ bins]{
\includegraphics[height=6.0cm]{img/soccer_orange_15625}
\label{fig:3d}
}
\caption{Each graph shows the performance of the five different color models
for a number of bins on the soccer movie when tracking the orange player. All
four graphs show a very distinctive pattern on the color models that lost
track.}
\label{fig:soccer}
\end{figure}

Figure \ref{fig:2a} shows that several color models have completely lost track,
indicating that $64$ bins is not enough. In \ref{fig:2b} only the $xyz$ model failed.
In both \ref{fig:2c} and \ref{fig:2d} all color models show good performance.
This indicates that the more bins used, the better the model becomes.

\begin{figure}[h!]
\centering
\subfigure[$64$ bins]{
\includegraphics[height=6.0cm]{img/earth_cheetah_64}
\label{fig:2a}
}
\subfigure[$729$ bins]{
\includegraphics[height=6.0cm]{img/earth_cheetah_729}
\label{fig:2b}
}
\\
\subfigure[$4096$ bins]{
\includegraphics[height=6.0cm]{img/earth_cheetah_4096}
\label{fig:2c}
}
\subfigure[$15625$ bins]{
\includegraphics[height=6.0cm]{img/earth_cheetah_15625}
\label{fig:2d}
}
\caption{Each graph shows the performance of the five different color models for
a number of bins on the cheetah movie when tracking the cheetah's head.
\ref{fig:2a} shows that several color models have completely lost track. In
\ref{fig:2b} only the $xyz$ model failed. In both \ref{fig:2c} and \ref{fig:2d}
all color models show good performance.}
\label{fig:cheetah}
\end{figure}

\section{Conclusions} \label{sec:conclusion}
This report introduced an implementation of the mean shift tracking algorithm
together with various color models applied to videos from two different domains.
From the results we conclude that a significant histogram size should be chosen
to represent a model for robust tracking. An insufficient amount of bins
overgeneralizes the model, at which point the tracker loses its target object as
both the target and background are (according to the model) indistinguishable.
Furthermore, the best color model, according to our results, is the normalized
$rgb$ model. That is, the $rgb$ color model had the best overall performance
across the domains and various histogram sizes.

\section{Future Work} \label{sec:future}
A lot can be improved in the current implementation. We will discuss some of
these improvements in this section.

\paragraph{Scale Invariance} The current implementation is not scale invariant. This
results in tracking problems when an object is getting closer or further away
from the camera or when a camera zooms in or out. In these cases the target
model gets outdated and the eventually the tracker will fail. Implementing
scale invariance will solve this problem. This can be done by computing the
Epanechnikov kernel for various scales and select the best scale for each
frame using the Bhattacharyya distance.

\paragraph{Kalman Filter} When the target object gets occluded by another object
or some other part of the scene the tracker might lose the target object. In
general the tracker tends to jitter a lot while tracking the target object.
It's likely that applying a Kalman Filter will solve both these problems. By
doing so, an assumption about the expected motion (i.e. the expected location
of the target object in the next frame) is made and therefore the tracker would
become much more stable.

\paragraph{Color Space} The tracker asumes a color model has a priori been
chosen. We suspect that the best choice for a color model depends on the domain
or the scene. It might be a good idea to implement some automated analysis to
determine which color space is best suited for the target model for every
induividual scene. For example the color space model with the highest variance
could be chosen or the color model in which the target object differs most from
the rest of the scene.

\paragraph{Spatial Layout} To improve the target model (especially when complex
target objects have to be tracked) one could divide the kernel into several
tiles to store some information about the spatial layout of the target object
to create more discriminative models. One should use caution with respect to
the problem of overfitting.

\renewcommand\bibname{References}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{document}
